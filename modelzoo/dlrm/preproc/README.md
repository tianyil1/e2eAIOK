### Requirements
    pandas 
    numpy
    pyarrow

### Before Run
* Raw data should be put under `/dlrm/input` in HDFS, or other path set for `INPUT_PATH` in `run_spark.sh`.
* Make sure `USE_FREQUENCY_LIMIT=1` in `run_spark.sh`.
* Adjust the settings for memory/cores in `run_spark.sh` according to your actual environment.
* Make the directories as below or according to your settings for `convert_output_folder` in `spark_data_utils.py`. 
  This should be done for each spark worker node. Previous output data under these dirs should be dropped before re-processing.
```
  /mnt/DP_disk5/test/
  /mnt/DP_disk5/train/
  /mnt/DP_disk5/validation/
```
* Test raw data and validation raw data are generated by splitting `day_23`. They can be reused by copying to current HDFS or 
  manually uploaded to `test_input_folder/validation_input_folder` after the split.

### How to Run
  `sh run_spark.sh`
  
### Original Code Link
https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM/preproc