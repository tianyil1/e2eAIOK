{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install pyrecdp from github"
      ],
      "metadata": {
        "id": "hNOO3-I-Tgzd"
      },
      "id": "hNOO3-I-Tgzd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736fb211-dbe6-4ca9-a1b1-db2cff2d287a",
      "metadata": {
        "id": "736fb211-dbe6-4ca9-a1b1-db2cff2d287a"
      },
      "outputs": [],
      "source": [
        "!pip install 'git+https://github.com/intel/e2eAIOK.git#egg=pyrecdp&subdirectory=RecDP'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install jdk for pyspark running"
      ],
      "metadata": {
        "id": "Jba8TLMvTn-G"
      },
      "id": "Jba8TLMvTn-G"
    },
    {
      "cell_type": "code",
      "source": [
        "!DEBIAN_FRONTEND=noninteractive apt-get install -y openjdk-8-jre"
      ],
      "metadata": {
        "id": "2aCojHZ6TQJE"
      },
      "id": "2aCojHZ6TQJE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare test data"
      ],
      "metadata": {
        "id": "3UMkbvwwT2Vc"
      },
      "id": "3UMkbvwwT2Vc"
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content/test_data\n",
        "%cd /content/test_data\n",
        "!wget https://raw.githubusercontent.com/intel/e2eAIOK/main/RecDP/tests/data/llm_data/arxiv_sample_100.jsonl"
      ],
      "metadata": {
        "id": "d6fX0InLTTXt"
      },
      "id": "d6fX0InLTTXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import quality classifier function"
      ],
      "metadata": {
        "id": "yAscO0gQWBKa"
      },
      "id": "yAscO0gQWBKa"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyrecdp.primitives.llmutils import quality_classifier_spark, quality_classifier\n",
        "from pyrecdp.core import SparkDataProcessor"
      ],
      "metadata": {
        "id": "QU2vJ3pWWIaF"
      },
      "id": "QU2vJ3pWWIaF",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify variables"
      ],
      "metadata": {
        "id": "Y248QUCGWUeL"
      },
      "id": "Y248QUCGWUeL"
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/test_data/arxiv_sample_100.jsonl'\n",
        "save_path = '/content/test_data/output/quality_classifier'"
      ],
      "metadata": {
        "id": "Oe9BDy0xWiVz"
      },
      "id": "Oe9BDy0xWiVz",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "UBhY9MxDYFN3"
      },
      "id": "UBhY9MxDYFN3"
    },
    {
      "cell_type": "code",
      "source": [
        "rdp = SparkDataProcessor()\n",
        "spark = rdp.spark\n",
        "spark_df = spark.read.json(data_file)\n",
        "spark_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_lfGGQOYD3F",
        "outputId": "39f68af9-5d7a-47de-8954-53c0d841b20b"
      },
      "id": "C_lfGGQOYD3F",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will assign 1 cores and 10386 M memory for spark\n",
            "per core memory size is 10.143 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
            "+--------------------+--------------------+\n",
            "|                meta|                text|\n",
            "+--------------------+--------------------+\n",
            "|{2203.15369, en, ...|\\section{Introduc...|\n",
            "|{math/9807097, en...|\\section{Introduc...|\n",
            "|{2008.06948, en, ...|\\section{Introduc...|\n",
            "|{cond-mat/9807071...|\\section{Introduc...|\n",
            "|{2210.10650, en, ...|\\section{\\label{s...|\n",
            "|{astro-ph/9807119...|\\section{Introduc...|\n",
            "|{2111.03152, en, ...|\\section{Introduc...|\n",
            "|{1606.04992, en, ...|\\n\\n\\section{Intr...|\n",
            "|{1608.03404, en, ...|\\section{introduc...|\n",
            "|{1904.10101, en, ...|\\section{Introduc...|\n",
            "|{cond-mat/9807275...|\\section{Introduc...|\n",
            "|{2109.05334, en, ...|\\section{Introduc...|\n",
            "|{1512.06966, en, ...|\\section{Introduc...|\n",
            "|{2112.04926, en, ...|\\section{Introduc...|\n",
            "|{2202.01000, en, ...|\\section{Introduc...|\n",
            "|{2209.13421, en, ...|\\section{Introduc...|\n",
            "|{1103.5603, en, 2...|\\section{Introduc...|\n",
            "|{1001.3679, en, 2...|\\section{Introduc...|\n",
            "|{1702.08222, en, ...|\\section{Introduc...|\n",
            "|{2201.05495, en, ...|\\section{Introduc...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the 'text' column and generate doc score which will determine whether the row kept or not(spark dataframe interface)"
      ],
      "metadata": {
        "id": "AQELlnB5WyeX"
      },
      "id": "AQELlnB5WyeX"
    },
    {
      "cell_type": "code",
      "source": [
        "quality_classifier_df = quality_classifier_spark(spark_df)\n",
        "quality_classifier_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgGPeSVZWwzZ",
        "outputId": "0aac93cc-04b2-4f2d-fa54-29bb47d95fb7"
      },
      "id": "EgGPeSVZWwzZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-08 01:27:03.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mprepare_model\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mPreparing scorer model in [/root/.cache/recdp/models/gpt3_quality_model]...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_name is gpt3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-08 01:27:18.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mStart scoring dataset...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----------+\n",
            "|                meta|                text|           doc_score|should_keep|\n",
            "+--------------------+--------------------+--------------------+-----------+\n",
            "|{2203.15369, en, ...|\\section{Introduc...|  0.9999999999996102|          1|\n",
            "|{math/9807097, en...|\\section{Introduc...|                 1.0|          1|\n",
            "|{2008.06948, en, ...|\\section{Introduc...|  0.9015197498942397|          0|\n",
            "|{cond-mat/9807071...|\\section{Introduc...|    0.99971976667992|          1|\n",
            "|{2210.10650, en, ...|\\section{\\label{s...|                 1.0|          1|\n",
            "|{astro-ph/9807119...|\\section{Introduc...|0.001319136449002...|          0|\n",
            "|{2111.03152, en, ...|\\section{Introduc...|                 0.0|          0|\n",
            "|{1606.04992, en, ...|\\n\\n\\section{Intr...|                 1.0|          1|\n",
            "|{1608.03404, en, ...|\\section{introduc...|                 1.0|          1|\n",
            "|{1904.10101, en, ...|\\section{Introduc...|  0.9999979720467516|          1|\n",
            "|{cond-mat/9807275...|\\section{Introduc...|                 1.0|          1|\n",
            "|{2109.05334, en, ...|\\section{Introduc...|3.005982129877793...|          0|\n",
            "|{1512.06966, en, ...|\\section{Introduc...|  0.9999752338514555|          1|\n",
            "|{2112.04926, en, ...|\\section{Introduc...|  0.9999998492093729|          1|\n",
            "|{2202.01000, en, ...|\\section{Introduc...|                 1.0|          1|\n",
            "|{2209.13421, en, ...|\\section{Introduc...|                 1.0|          1|\n",
            "|{1103.5603, en, 2...|\\section{Introduc...|  0.9999999999995527|          1|\n",
            "|{1001.3679, en, 2...|\\section{Introduc...|                 0.0|          0|\n",
            "|{1702.08222, en, ...|\\section{Introduc...|  0.9999988629258854|          1|\n",
            "|{2201.05495, en, ...|\\section{Introduc...|5.299151714099892E-7|          0|\n",
            "+--------------------+--------------------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the 'text' column and generate doc score and save data as parquet format."
      ],
      "metadata": {
        "id": "fa3J-ZV8hQ27"
      },
      "id": "fa3J-ZV8hQ27"
    },
    {
      "cell_type": "code",
      "source": [
        "quality_classifier(data_file, save_path, data_file_type=\"jsonl\")\n",
        "!ls $save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRzUf0ScYZae",
        "outputId": "340c8820-8ba1-4466-a2ba-1f7c8758d390"
      },
      "id": "QRzUf0ScYZae",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init spark\n",
            "Will assign 1 cores and 10386 M memory for spark\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-08 01:29:36.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mprepare_model\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mPreparing scorer model in [/root/.cache/recdp/models/gpt3_quality_model]...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "per core memory size is 10.143 GB and shuffle_disk maximum capacity is 8589934592.000 GB\n",
            "execute with spark started ...\n",
            "model_name is gpt3\n",
            "real_model_path is /root/.cache/recdp/models/gpt3_quality_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-08 01:29:39.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyrecdp.primitives.operations.text_qualityscorer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mStart scoring dataset...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execute with spark took 11.547022748000018 sec\n",
            "part-00000-088e9ef7-8cf8-4987-a50b-a1775936ce5c-c000.snappy.parquet  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aGEqMQT59v_"
      },
      "id": "-aGEqMQT59v_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}